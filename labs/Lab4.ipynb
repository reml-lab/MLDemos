{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Subgradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement linear regression with basis expansion using a custom non-differnetiable convex loss function. The lab will take you though the steps needed to implement the loss, the loss sub-gradient, the regularized risk and the regularized risk sub-gradient, including steps that require some derivations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Optimizer & Plotting\n",
    "This section defines imports, the subgradient decent optimizer that will use, along with supporting plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def subgradient_descent(x0,f,g,args=[],alpha=1,maxIter=100,tol=1e-4,callback=None):\n",
    "    '''\n",
    "    Args:\n",
    "        x0: Starting value for optimizer. Shape (D,1)\n",
    "        f: Objective functions. Accepts input of shape (D,1)\n",
    "        g: Subgradient function for f. Accepts input of shape (D,1)\n",
    "        alpha: Starting step size\n",
    "        maxIter: Maximum number of optimization iterations\n",
    "        tol: Convergence tolerance. \n",
    "        callback: Called after each iteration with current x and f(x) values\n",
    "\n",
    "    Returns:\n",
    "        Tuple consisting of (xbest,fbest), the best value of x found and the value of f(xbest)\n",
    "     \n",
    "    '''\n",
    "    D     = x0.shape[0]\n",
    "    fbest = np.inf\n",
    "    flast = None\n",
    "    xbest = x0\n",
    "    x     = x0\n",
    "\n",
    "    for i in range(maxIter - 1):\n",
    "        gi = g(x,*args)\n",
    "        alpha_i = (alpha / (1 + np.sqrt(i))) / (1+np.sqrt(np.sum(gi**2)))\n",
    "        x  = x - alpha_i * gi\n",
    "        fi  = f(x,*args)\n",
    "\n",
    "        if(callback is not None):\n",
    "            callback(x,fi)\n",
    "\n",
    "        if fi < fbest:\n",
    "            xbest = x\n",
    "            fbest = fi\n",
    "\n",
    "        if(i%(maxIter//10)==0): print(f\"Iteration {i}: obj: {fi:.4f}\")\n",
    "\n",
    "        if flast is not None and  abs(fi - flast) / (1e-8+abs(flast)) < tol: break\n",
    "        flast = fi\n",
    "\n",
    "\n",
    "    return(xbest, fbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Loss Function Implementation\n",
    "This section defines the regression loss function $\\ell_{\\epsilon}(r)$ that we'll use for this exercise in terms of the residual $r_n=y_n-f_{\\theta}(\\mathbf{x}_n)$. Your first task is to implement this loss function. Your implementation should be vectorized so that it takes a column vector of residuals as input and produces a column vector of loss values as output. The function is defined as follows:\n",
    "\n",
    "$$\\ell_{\\epsilon}(r) = \\max(0, r^2-\\epsilon^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(r,eps):\n",
    "    out = r*0 #Add code here\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the loss function implemented, run the code below to plot the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "r = np.linspace(-5,5,100).reshape(-1,1)\n",
    "lr = l(r,2)\n",
    "plt.plot(r,lr,\"b-\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"l(r)\")\n",
    "plt.title(\"Loss Function Plot (eps=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Convexity\n",
    "Using properties of convex functions, prove that this loss function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Subgradient\n",
    "Derive a subgradient function $\\ell'_{\\epsilon}(r)$ for the loss function $\\ell_{\\epsilon}(r)$. If the subdifferential is not a singleton set for a given value of $r$ and $0$ is in the sub-differential at $r$, use $0$ as the subgradient value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Sub-gradient Implementation\n",
    "Implement your sub-gradient function $\\ell'_{\\epsilon}(r)$, then use the supplied code to plot it. Your implementation must accept a column vector of residual values as input and produce a corresponding column vector of sub-gradient values as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lprime(r,eps):\n",
    "    out = r*0 #Add code here\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the loss function implemented, run the code below to plot the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "r = np.linspace(-5,5,1000).reshape(-1,1)\n",
    "plt.plot(r,lprime(r,2),\"b-\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"l'(r)\")\n",
    "plt.title(\"Loss Sub-gradient Function (eps=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Regularized Risk\n",
    "We are now going to learn a linear regression model using this loss function. We will use a regularized risk with the squared two norm regularizer as shown below. You need to implement the regularized risk function. It must take as input an array of parameter values $\\theta=[\\mathbf{w};b]$ of shape (D+1,1), an array of bias absorbed inputs $X$ of shape (N,D+1), an array of outputs $Y$ of shape (N,1), and a value of the regularization parameter $\\lambda$. The implementation should be vectorized and should call your implementation of the loss function $\\ell_{\\epsilon}(r)$.\n",
    "$$R(\\theta,D) = \\frac{1}{N}\\sum_{n=1}^N \\ell_{\\epsilon}(y_n-\\mathbf{x}_n\\theta) + \\lambda \\Vert\\mathbf{w}\\Vert_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_risk(theta, X, Y, eps,lam):\n",
    "    out = 0 #Add code here\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Regularized Risk Sub-Gradient\n",
    "The final component we need to learn the model is a regularized risk sub-gradient function. For this problem, the regularized risk sub-gradient is given by the expression below. You next need to implement this function. It must take as input an array of parameter values $\\theta=[\\mathbf{w};0]$ of shape (D+1,1), an array of bias absorbed inputs $X$ of shape (N,D+1), an array of outputs $Y$ of shape (N,1), and a value of the regularization parameter $\\lambda$. The implementation should be vectorized and should call your implementation of the loss subgradiejt function $\\ell'_{\\epsilon}(r)$.\n",
    "\n",
    "$$-\\frac{1}{N}\\sum_{n=1}^N \\ell'_{\\epsilon}(y_n-\\mathbf{x}_n\\theta)\\mathbf{x}_n^T + 2\\lambda[\\mathbf{w};0]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_risk_gradient(theta, X, Y, eps,lam):\n",
    "    out = np.zeros((X.shape[1],1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Effect of $\\epsilon$ on Learning\n",
    "We are now going to apply the sub-gradient descent learning procedure to learn the model using your regularized risk and sub-gradient functions. First, we'll construct a synthetic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "B = 2\n",
    "X = 10*(np.random.rand(N,1)-0.5)\n",
    "Y = np.cos(X) +  np.random.randn(N,1)/3\n",
    "plt.plot(X,Y,\"b.\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Data Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn the model using a polynomial basis expansion of order 5. We define some helper functions below to perform the basis expansion, standardize the data and apply bias absorption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X,B):\n",
    "    return X**np.arange(1,B+1)\n",
    "\n",
    "def standardize(X,m=None,s=None):\n",
    "    if m is None:\n",
    "        m=X.mean(axis=0)\n",
    "    if s is None:\n",
    "        s=X.std(axis=0)\n",
    "    return (X-m)/s, m, s\n",
    "\n",
    "def bias_absorb(X):\n",
    "    return np.hstack([X, np.ones((X.shape[0],1))])\n",
    "\n",
    "def featurize(X,B,m=None,s=None):\n",
    "    XB = phi(X,B)\n",
    "    XB,m,s  = standardize(XB,m,s)\n",
    "    XB =  bias_absorb(XB)\n",
    "    return XB,m,s\n",
    "\n",
    "B       = 5\n",
    "XB,m,s  = featurize(X,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run the optimizer and plot the results. We start with $\\epsilon=0$, which results in a special case of the loss function that is equivalent to OLS. Leaving the other parameters as is, try increasing the value of $\\epsilon$. Explain what happens to the learned prediction function as $\\epsilon$ is increased. Why does this learning procedure exhibit this behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps     = 0\n",
    "lam     = 0.01\n",
    "theta0 = np.zeros((B+1,1))\n",
    "theta_star, R_star = subgradient_descent(theta0,regularized_risk,regularized_risk_gradient,[XB,Y,eps,lam],tol=1e-8,maxIter=1000,alpha=1)\n",
    "\n",
    "N = 100\n",
    "XT  = np.linspace(-5,5,100).reshape(100,1)\n",
    "XBT,_,_ = featurize(XT,B,m,s)\n",
    "YT = XBT@theta_star\n",
    "plt.plot(X,Y,\"b.\")\n",
    "plt.plot(XT[:,0],YT,\"k-\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Data Set\")\n",
    "plt.legend([\"Data\",\"Prediction Function\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Learnability of $\\epsilon$\n",
    "Can the $\\epsilon$ parameter of the loss be chosen based on the resulting value of the regularized risk? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
